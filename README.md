## This is the accompanying code for the paper: Bottlenecks and Solutions for Audio to Score Alignment Research

It is a repository that allows the easy application of several DTW variants for audio to score alignment on an adapted version of the asap dataset, as explained in the publication. 
 
The purpose of this is to create a foundation for a system which can be used for Audio to Score alignment benchmarking. At its current state it is by no means complete, but it is a call for interested researchers to gather and discuss what should be done. For this to be a framework for benchmarking, we would need:
1) to extend the datasets used
2) to include more audio to score alignment implementations, including HMM based ones and others.
3) to expand the evaluation metrics, to cover a variety of scenarios, as argued in the initial publication.

Examples of relevant datasets: 
TO COMPLETE: Relevant datasets for Audio to Score alignment

Saarland		Granularity  Stength 
RWC alignment subset
Phoenix
Bach10
ASAP (vanilla)
Interpolated ASAP

Granularity Strength Ideas
beat level
note level

The paper discusses the use of just one of such datasets (the ASAP dataset), for Audio to Score alignment.  


## Setting up
Most of the requirements of this repository would be included in the environment.yml of the docker subdirectory. However, there are some things that (for now) need to be installed manually.

https://github.com/christofw/pitchclass_mctc (this codebase is on commit 04b4a96)

## Extending the codebase
To add a new dataset

To add a new algorithm

To include training as part of this codebase

- if you are to add another dtw implementation, make sure that you transpose the audio representations to the format expected by the distance measure used.
- On the conversion from and to the actual times,

## Running with Docker
To run with Docker, please see docker/README for instructions on how to build a docker image from the resources in this repository, and how to mount folders on your local machine to the docker container (to avoid the image getting extra large, and so that the data persists after calculation)

On completing the said instructions, you would have a docker container with a copy of asa_benchmarks, in the root directory, where the data, align, and eval folders are symlinks to their corresponding folders in /mnt of the container (which are in turn folders in your local asa_benchmarks). Hence, you can run everything in the docker container, and the data would persist in their local locations in asa_benchmarks.
 
## Getting the ASAP dataset

However, first the ASAP files need to be restructured a bit.

```
from extract import restructure_asap_files

restructure_asap_files('/Users/aliamorsi/Desktop/phD/a2s_with_dtw_survey/pitchclass_mctc/data/asap-preludes', 'metadata.csv')
```

## Interpolating Ground Truths from ASAP

We have added the ability to both generate ground truth annotations and to sonify them + play them alongside the actual performance wav. 

Then, run the following to create the ground truth interpolations

```
python3 align.py ground-beat-interpol data/score data/perf 0
```

To sonify them, run:

```
from extract import sonify_interpolated_gt

sonify_interpolated_gt()
```

and by default, the sonified files will be created in eval/sonic.
These files are stereo, with the sonified performance_aligned_gt on one channel and the performance wav on the other.

Open them with audacity, split to have each channel on a separate track, and play with the volume so that they sound even. If playing them together does not seem odd, then there is no problem with the ground truth.

## Computing Alignments

To compute an alignment with a given dataset, 
```
python3 align.py {spectra,chroma,cqt} data/score data/perf N
```

The alignments generated by the alignment script are stored in align/{ground,spectra,chroma,cqt} as
plaintext files with two columns: the first column indicates time in the score, and the second
column indicates time in the performance.

To evaluate the results of a particular alignment algorithm:

(The evaluate and the generation of result files should be one step..)

```
python3 eval.py {spectra,chroma,cqt,ctc-chroma} data/score data/perf
```

To generate result files from the alignment process, run

```
from eval import calculate_bulk_metrics

calculate_bulk_metrics('align/ctc-chroma/', 'align/ground-beat-interpol', 'data/score', 'data/perf')
```

To create the extra metadata which we will use to filter things later

```
from eval import metadata_for_qa

metadata_for_qa('data/score', 'data/perf', 'align/ground-beat-interpol')
```

## Visualizations 

TODO: Add reference to the musicxml visualizations, and keep the visualizations by Thickstun + relate them to the missing notes/extra notes evaluation by Nakamura. 


## References

To reference this work, please cite
```bib
@article{morsi_serra_bottlenecks, 
author = {Alia Morsi and Xavier Serra}, 
title = {Bottlenecks and Solutions for Audio to Score Alignment Research}, 
journal = {Proceedings of the 23rd International Society for Music Information Conference (ISMIR)},
year = {2022}
}
```

Although it has diverged, this repo started out as a fork of https://github.com/jthickstun/alignment-eval


