## This is the accompanying code for the paper: Breaking the Glass Ceiling

The purpose of this is to create a foundation for a system which can be used for Audio to Score alignment benchmarking. At its current state it is by no means complete, but it is a call for interested researchers to gather and discuss what should be done. 

Relevant datasets for Audio to Score alignment

Saarland		Granularity  Stength 
RWC alignment subset
Phoenix
Bach10
ASAP (vanilla)
Interpolated ASAP

Granularity Strength Ideas
beat level
note level

The paper discusses the use of just one of such datasets (the ASAP dataset), for Audio to Score alignment.  


## Extending the codebase
To add a new dataset

To add a new algorithm

To include training as part of this codebase

- if you are to add another dtw implementation, make sure that you transpose the audio representations to the format expected by the distance measure used.
- On the conversion from and to the actual times,

## Running with Docker
To run with Docker, please see docker/README for instructions on how to build a docker image from the resources in this repository, and how to mount folders on your local machine to the docker container (to avoid the image getting extra large, and so that the data persists after calculation)

On completing the said instructions, you would have a docker container with a copy of alignment-eval, in the root directory, where the data, align, and eval folders are symlinks to their corresponding folders in /mnt of the container (which are in turn folders in your local alignment-eval). Hence, you can run everything in the docker container, and the data would persist in their local locations in alignment_eval.
 
## Getting the ASAP dataset

However, first the ASAP files need to be restructured a bit.

```
from extract import restructure_asap_files

restructure_asap_files('/Users/aliamorsi/Desktop/phD/a2s_with_dtw_survey/pitchclass_mctc/data/asap-preludes', 'metadata.csv')
```

## Interpolating Ground Truths from ASAP

We have added the ability to both generate ground truth annotations and to sonify them + play them alongside the actual performance wav. 

Then, run the following to create the ground truth interpolations

```
python3 align.py ground-beat-interpol data/score data/perf 0
```

To sonify them, run:

```
from extract import sonify_interpolated_gt

sonify_interpolated_gt()
```

and by default, the sonified files will be created in eval/sonic.
These files are stereo, with the sonified performance_aligned_gt on one channel and the performance wav on the other.

Open them with audacity, split to have each channel on a separate track, and play with the volume so that they sound even. If playing them together does not seem odd, then there is no problem with the ground truth.

## Computing Alignments

To compute an alignment with a given dataset, 
```
python3 align.py {spectra,chroma,cqt} data/score data/perf N
```

The alignments generated by the alignment script are stored in align/{ground,spectra,chroma,cqt} as
plaintext files with two columns: the first column indicates time in the score, and the second
column indicates time in the performance.

To evaluate the results of a particular alignment algorithm:

(The evaluate and the generation of result files should be one step..)

```
python3 eval.py {spectra,chroma,cqt,ctc-chroma} data/score data/perf
```

To generate result files from the alignment process, run

```
from eval import calculate_bulk_metrics

calculate_bulk_metrics('align/ctc-chroma/', 'align/ground-beat-interpol', 'data/score', 'data/perf')
```

To create the extra metadata which we will use to filter things later

```
from eval import metadata_for_qa

metadata_for_qa('data/score', 'data/perf', 'align/ground-beat-interpol')
```

## Visualizations 

(There is something here which could be related to the evaluation of Audio to Score alignment as per the Eita Nakamura paper)
To understand the behavior of the ground-truth alignments, we can visually compare the piano-roll
performance (subplot 1) captured by the Yamaha Disklavier to the performance-aligned score created
by warping the score according to the ground-truth alignment (subplot 2). In the comparison plot
(subplot 3) we use red to identify notes that are indicated by the performance-aligned score but
not performed and yellow to identify notes that are performed but not indicated by the
performance-aligned score. This example visualizes the beginning of a performance of the Bach's
Prelude and Fugue in G-sharp minor (BWV 863).

![](assets/ground_truth.png)

We can also use these visualizations to compare the results of an candidate alignment algorithm to
the ground-truth alignment. In each case, red is used to identify notes that are indicated by the
candidate alignment algorithm, but not by the ground-truth alignment, and yellow is used to
identify notes that are indicated by the ground-truth alignment, but not by the candidate
alignment.

![](assets/candidates.png)

## References

To reference this work, please cite

```bib
@article{thickstun2020rethinking,
  author    = {John Thickstun and Jennifer Brennan and Harsh Verma},
  title     = {Rethinking Evaluation Methodology for Audio-to-Score Alignment},
  journal   = {arXiv preprint arXiv:2009.14374},
  year      = {2020},
}
```


